{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4rXx+IBmk/gqvqpKeLkJU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay05Hawk/Keras_tutorial/blob/main/Keras_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Keras}}$ \n",
        "Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result as fast as possible is key to doing good research.\n",
        "\n",
        "Keras is:\n",
        "\n",
        "$\\color{blue}{\\text{Simple--}}$  but not simplistic. Keras reduces developer cognitive load to free you to focus on the parts of the problem that really matter.\n",
        "\n",
        "$\\color{blue}{\\text{Flexible --}}$ Keras adopts the principle of progressive disclosure of complexity: simple workflows should be quick and easy, while arbitrarily advanced workflows should be possible via a clear path that builds upon what you've already learned.\n",
        "\n",
        "$\\color{blue}{\\text{Powerful --}}$ Keras provides industry-strength performance and scalability: it is used by organizations and companies including NASA, YouTube, and Waymo."
      ],
      "metadata": {
        "id": "NTFT5yza3hr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##$\\color{blue}{\\text{Optimizers}}$\n",
        "\n",
        "- SGD\n",
        "- RMSprop\n",
        "- Adam\n",
        "- AdamW\n",
        "- AdaDelta\n",
        "- Ada Grade\n",
        "- AdaMax\n",
        "- AdaFactor\n",
        "- Nadam\n",
        "- Ftrl\n",
        "\n",
        "\n",
        "$\\color{blue}{\\text{Usage with compile() & fit()}}$ \n",
        "\n",
        "An optimizer is one of the two arguments required for compiling a Keras model:"
      ],
      "metadata": {
        "id": "fnmjT0Hy3jZm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kQwiBVop3aN-"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
        "model.add(layers.Activation('softmax'))\n",
        "\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass optimizer by name: default parameters will be used\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "yVWc4lz93mFe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Usage in a custom training loop\n",
        "When writing a custom training loop, you would retrieve gradients via a$\\color{blue}{\\text{tf.GradientTape}}$  instance, then call optimizer. $\\color{blue}{\\text{apply_gradients()}}$ to update your weights:"
      ],
      "metadata": {
        "id": "8Y_N577C-buZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Instantiate an optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Iterate over the batches of a dataset.\n",
        "for x, y in dataset:\n",
        "    # Open a GradientTape.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass.\n",
        "        logits = model(x)\n",
        "        # Loss value for this batch.\n",
        "        loss_value = loss_fn(y, logits)\n",
        "\n",
        "    # Get gradients of loss wrt the weights.\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "    # Update the weights of the model.\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))"
      ],
      "metadata": {
        "id": "x1F3Bvb83mCm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJUR2XYr3l_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning rate decay / scheduling\n",
        "You can use a learning rate schedule to modulate how the learning rate of your optimizer changes over time:"
      ],
      "metadata": {
        "id": "5cHO2M2---ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=1e-2,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=0.9)\n",
        "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)"
      ],
      "metadata": {
        "id": "QAL3uKfM3l8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###apply_gradients method\n",
        "Arguments\n",
        "\n",
        "- grads_and_vars: List of (gradient, variable) pairs.\n",
        "\n",
        "- name: string, defaults to None. The name of the namescope to use when creating variables. If None, self.name will be used.\n",
        "\n",
        "- skip_gradients_aggregation: If true, gradients aggregation will not be \n",
        "performed inside optimizer. Usually this arg is set to True when you write custom code aggregating gradients outside the optimizer.\n",
        "\n",
        "- **kwargs: keyword arguments only used for backward compatibility.\n",
        "Returns\n",
        "\n",
        "A tf.Variable, representing the current iteration."
      ],
      "metadata": {
        "id": "ShCpgMlB_-Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Optimizer.apply_gradients(\n",
        "    grads_and_vars, name=None, skip_gradients_aggregation=False, **kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "j3PdWCIW3l5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Optimizer.variables()\n",
        "# Returns variables of this optimizer."
      ],
      "metadata": {
        "id": "evKzmfmb3l2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Losses\n",
        "The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\n",
        "\n",
        "Available losses\n",
        "Note that all losses are available both via a class handle and via a function handle. The class handles enable you to pass configuration arguments to the constructor (e.g. loss_fn = CategoricalCrossentropy(from_logits=True)), and they perform reduction by default when used in a standalone way (see details below).\n",
        "\n",
        " $\\color{blue}{\\text{Probabilistic losses}}$ for classificaon problems\n",
        "- BinaryCrossentropy class\n",
        "- CategoricalCrossentropy class\n",
        "- SparseCategoricalCrossentropy class\n",
        "- Poisson class\n",
        "- binary_crossentropy function\n",
        "- categorical_crossentropy function\n",
        "- sparse_categorical_crossentropy function\n",
        "- poisson function\n",
        "- KLDivergence class\n",
        "- kl_divergence function\n",
        "\n",
        "$\\color{blue}{\\text{Regression  losses}}$ For regression problem\n",
        "- MeanSquaredError class\n",
        "- MeanAbsoluteError class\n",
        "- MeanAbsolutePercentageError class\n",
        "- MeanSquaredLogarithmicError class\n",
        "- CosineSimilarity class\n",
        "- mean_squared_error function\n",
        "- mean_absolute_error function\n",
        "- mean_absolute_percentage_error function\n",
        "- mean_squared_logarithmic_error function\n",
        "- cosine_similarity function\n",
        "- Huber class\n",
        "- huber function\n",
        "- LogCosh class\n",
        "- log_cosh function\n",
        "\n",
        "$\\color{blue}{\\text{Hinge losses for \"maximum-margin\" classification}}$\n",
        "- Hinge class\n",
        "- SquaredHinge class\n",
        "- CategoricalHinge class\n",
        "- hinge function\n",
        "- squared_hinge function\n",
        "- categorical_hinge function\n"
      ],
      "metadata": {
        "id": "SJZvbqPHBBBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{blue}{\\text{Data loading  }}$\n",
        "Keras data loading utilities, located in tf.keras.utils, help you go from raw data on disk to a tf.data.Dataset object that can be used to efficiently train a model.\n",
        "\n",
        "These loading utilites can be combined with preprocessing layers to futher transform your input dataset before training.\n",
        "\n",
        "\n",
        "[Read It for more info](https://keras.io/api/data_loading/)\n",
        "\n"
      ],
      "metadata": {
        "id": "G5vk7NwSC68N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{blue}{\\text{KerasCV API  }}$\n",
        "KerasCV is a toolbox of modular building blocks (layers, metrics, losses, data augmentation) that computer vision engineers can leverage to quickly assemble production-grade, state-of-the-art training and inference pipelines for common use cases such as image classification, object detection, image segmentation, image data augmentation, etc.\n",
        "\n",
        "KerasCV Layers can be used independently, or with the keras.Model class. Layers implement specific self contained logic such as image data augmentation, regularization during training, and more!\n",
        "\n",
        "KerasCV Metrics are used for model evaluation. They can be used for both train time and post training evaluation. They can be used independently, or as part of the standard Model.fit(), Model.evaluate(), Model.predict() flow.\n",
        "\n",
        "\n",
        "[must Read It for more info](https://keras.io/api/keras_cv/)\n",
        "\n",
        "###Layers\n",
        "- Augmentation layers\n",
        "- Preprocessing layers\n",
        "- Regularization layers\n",
        "\n",
        "###Models\n",
        "- StableDiffusion image-generation model\n",
        "- The RetinaNet model\n",
        "- The FasterRCNN model\n",
        "- EfficientNetV2 models\n",
        "- DenseNet models\n",
        "\n",
        "###Bounding box formats and utilities\n",
        "- Bounding box formats\n",
        "- Bounding box utilities"
      ],
      "metadata": {
        "id": "7D7Yy04iD8fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{blue}{\\text{KerasNLP  }}$\n",
        "KerasNLP is a toolbox of modular building blocks ranging from pretrained state-of-the-art models, to low-level Transformer Encoder layers. For an introduction to the library see the KerasNLP home page. For a high-level introduction to the API see our getting started guide.\n",
        "\n",
        "\n",
        "[must Read It for more info](https://keras.io/api/keras_nlp/)\n",
        "\n",
        "##Models\n",
        "- Bert\n",
        "- DistilBert\n",
        "- Roberta\n",
        "- XLMRoberta\n",
        "\n",
        "##Tokenizers\n",
        "- Tokenizer base class\n",
        "- WordPieceTokenizer\n",
        "- SentencePieceTokenizer\n",
        "- BytePairTokenizer\n",
        "- ByteTokenizer\n",
        "- UnicodeCodepointTokenizer\n",
        "- compute_word_piece_vocabulary function\n",
        "- compute_sentence_piece_proto function\n",
        "\n",
        "##Preprocessing Layers\n",
        "- StartEndPacker layer\n",
        "- MultiSegmentPacker layer\n",
        "- RandomSwap layer\n",
        "- RandomDeletion layer\n",
        "- MaskedLMMaskGenerator layer\n",
        "\n",
        "\n",
        "##Modeling Layers\n",
        "- TransformerEncoder layer\n",
        "- TransformerDecoder layer\n",
        "- FNetEncoder layer\n",
        "- PositionEmbedding layer\n",
        "- SinePositionEncoding layer\n",
        "- TokenAndPositionEmbedding layer\n",
        "- MaskedLMHead layer\n",
        "\n",
        "##Metrics\n",
        "- Perplexity metric\n",
        "- RougeL metric\n",
        "- RougeN metric\n",
        "- Bleu metric\n",
        "- EditDistance metric\n",
        "\n",
        "##Utils\n",
        "- greedy_search function\n",
        "- top_k_search function\n",
        "- top_p_search function\n",
        "- random_search function\n",
        "- beam_search function\n",
        "\n",
        "[Examples NLP](https://keras.io/examples/nlp/)"
      ],
      "metadata": {
        "id": "0uIDWImPFE4f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TdDqCzzD3lzn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}